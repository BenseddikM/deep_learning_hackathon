{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Challenge Tenserflow - MDI341 2016/2017</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mohammed BENSEDDIK "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Preprocessing the data: </strong>\n",
    "* Used ImageAugmentation after the prepocessing of the data.\n",
    "Construction of the model:\n",
    "\n",
    "* Definition of the number of convolutions layer and fulling connected by respecting the number of allowed parameters\n",
    "<br\\>\n",
    "\n",
    "<strong> Choice of architecture: </strong> <br\\>\n",
    "The chosen model consists of alternating between layers of convolutions, pooling and activation. After selecting several parameters and several architectures, I chose the following architecture which gave me the best result.\n",
    "* The first convolution layer with a 4x4 patch and a stride (moving on the image) of one step and returns to output 10 filtered images of the same size as the input image.\n",
    "* Preprocessing the data and chosing an image augmentation in order to have better results.\n",
    "* The activation layers: The function relu (x -> max (0, x) according to the documentation it is the function that gives better results.\n",
    "* The pooling layer: The max_pooling which returns the maximum value on a 2X2 patch. This operation reduces the size of the image. In our case, we reduce the image by 48X48 ==> 24X24\n",
    "* The second convolution layer inputs 16 images from the previous layer and with a 4X4 filter, the layer generates 10 images filtered out.\n",
    "* One layer of Pooling: The function max_pooling reduces the image of 24X24 ==> 12X12\n",
    "* The third convolution layer takes as input 16 images of the previous output and with a 4X4 filter, the layer produces 10 filtered images.\n",
    "* One activation layer:\n",
    "* A layer of Pooling: The function max_pooling reduces the image of 12X12 ==> 6X6\n",
    "* A fully connected: which takes as input the output of the fourth pooling layer (6X6X10) put on a single vector. This vector is fully connected to a dimension output (128,1).\n",
    "* An out layer : (128,128).\n",
    "\n",
    "To avoid overfitting, I have added a dropout which allows to randomly disable, at each operation of optimization of the weights (backpropagation), a portion of connections between the nodes.\n",
    "\n",
    "<strong> Initializing weights: </strong>\n",
    "\n",
    "* According to the literature, it is desirable to initialize the weight matrix W according to a distribution of zero mean and standard deviation of 0.01.\n",
    "* The bias vector initialized to zero.\n",
    "\n",
    "<strong> Optimizing the loss function: </strong>\n",
    "\n",
    "* For the optimization part, I chose AdamOptimizer which allows to optimize the error by updating the weight matrix W by a backpropagation operation at each iteration. An iteration represents an image minibatch, in my case I chose a batch_size of 128.\n",
    "* At the beginning, I set a learning rate at 0.001. Afterwards, I dynamically varied it between 0.001 and 0.01 in such a way that learning rate decreases from one iteration to another. This operation allows a convergence Rapid at the beginning and after it decreases more slowly.\n",
    "\n",
    "\n",
    "However, the score obtained was not convincing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> KERAS </strong>\n",
    "<strong> Preprocessing the data: </strong>\n",
    "* Standardization of data.\n",
    "* Data Augmentation.\n",
    "<br\\>\n",
    "\n",
    "\n",
    "I considered an architecture slightly different from that presented previously. In fact it is a convolution layer, pooling\n",
    "And activation. The difference with tensor flow KERAS offers a function \"prelu\" more efficient than the replay function. The preluted function does not set the negative weight values ​​to zero but reduces them by multiplying them by a scalar learned by the model.\n",
    "<strong> CNN Architecture with KERAS: </strong>\n",
    "\n",
    "$$\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param    \n",
    "=================================================================\n",
    "Conv1 (Conv2D)               (None, 48, 48, 10)        170       \n",
    "_________________________________________________________________\n",
    "p_re_lu_4 (PReLU)            (None, 48, 48, 10)        10        \n",
    "_________________________________________________________________\n",
    "pool1 (AveragePooling2D)     (None, 24, 24, 10)        0         \n",
    "_________________________________________________________________\n",
    "dropout_5 (Dropout)          (None, 24, 24, 10)        0         \n",
    "_________________________________________________________________\n",
    "Conv2 (Conv2D)               (None, 24, 24, 10)        1610      \n",
    "_________________________________________________________________\n",
    "p_re_lu_5 (PReLU)            (None, 24, 24, 10)        10        \n",
    "_________________________________________________________________\n",
    "pool2 (AveragePooling2D)     (None, 12, 12, 10)        0         \n",
    "_________________________________________________________________\n",
    "dropout_6 (Dropout)          (None, 12, 12, 10)        0         \n",
    "_________________________________________________________________\n",
    "Conv3 (Conv2D)               (None, 12, 12, 10)        1610      \n",
    "_________________________________________________________________\n",
    "p_re_lu_6 (PReLU)            (None, 12, 12, 10)        10        \n",
    "_________________________________________________________________\n",
    "pool3 (AveragePooling2D)     (None, 6, 6, 10)          0         \n",
    "_________________________________________________________________\n",
    "dropout_7 (Dropout)          (None, 6, 6, 10)          0         \n",
    "_________________________________________________________________\n",
    "locally_connected2d_2 (Local (None, 1, 1, 128)         46208     \n",
    "_________________________________________________________________\n",
    "dropout_8 (Dropout)          (None, 1, 1, 128)         0         \n",
    "_________________________________________________________________\n",
    "flatten_2 (Flatten)          (None, 128)               0         \n",
    "=================================================================\n",
    "Total params: 49,628\n",
    "Trainable params: 49,628\n",
    "Non-trainable params: 0\n",
    "$$\n",
    "\n",
    "<strong> Optimization: </strong>\n",
    "\n",
    "Optimizer :\n",
    "* Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***************** Coding Part **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Rule\n",
    "\n",
    "For those of you willing to use a CNN with Tensorflow, to avoid giving advantages to participants with access to large amount of GPU/CPU, and to fit the application needs, we put a constraint on the total number of parameters to be learned.\n",
    "\n",
    "**The maximum number of parameters that your CNN can contain is limited to 50K! Again Tensorflow is mandatory in that case.** \n",
    "\n",
    "Whatever you use, at the end of the challenge you will need to submit your code along with your reports. If you use CNNs, you will also need to submit your model. Use your resources wisely!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "* This notebook is an adaptation of TF tutorial for the challenge. It shows how to construct a Tensor Flow pipeline without convolution etc ...\n",
    "\n",
    "* The idea is just to show how to save / restore a model and how to predict with this model.\n",
    "\n",
    "* Today the number of parameter is incredibly over the limit (see _Important Rule_).\n",
    "\n",
    "* The model performance in the Notebook is bad (worst than linear), but you can run it longer, this will probably convege ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "# import keras as kr\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tflearn.data_preprocessing import ImagePreprocessing\n",
    "from tflearn.data_augmentation import ImageAugmentation\n",
    "from tflearn.layers.core import input_data\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and investigating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training set\n",
    "\n",
    "path = 'data_folder/'\n",
    "\n",
    "images_train_fname    = path + 'data_train.bin'\n",
    "templates_train_fname = path + 'fv_train.bin'\n",
    "\n",
    "# Validation set\n",
    "images_valid_fname    = path + 'data_valid.bin'\n",
    "templates_valid_fname = path + 'fv_valid.bin'\n",
    "\n",
    "# Testing set\n",
    "images_test_fname     = path + 'data_test.bin'\n",
    "\n",
    "# number of images\n",
    "num_train_images = 100000\n",
    "num_valid_images = 10000\n",
    "num_test_images  = 10000\n",
    "\n",
    "# size of the images 48*48 pixels in gray levels\n",
    "image_dim = 48 * 48\n",
    "\n",
    "# dimension of the templates\n",
    "template_dim = 128\n",
    "\n",
    "# read the training files\n",
    "with open(images_train_fname, 'rb') as f:\n",
    "    X_train = np.fromfile(f, dtype=np.uint8, count=num_train_images * image_dim).astype(np.float32)\n",
    "    X_train = X_train.reshape(num_train_images, image_dim)\n",
    "\n",
    "with open(templates_train_fname, 'rb') as f:\n",
    "    Y_train = np.fromfile(f, dtype=np.float32, count=num_train_images * template_dim)\n",
    "    Y_train = Y_train.reshape(num_train_images, template_dim)\n",
    "\n",
    "# read the validation files\n",
    "with open(images_valid_fname, 'rb') as f:\n",
    "    X_valid = np.fromfile(f, dtype=np.uint8, count=num_valid_images * image_dim).astype(np.float32)\n",
    "    X_valid = X_valid.reshape(num_valid_images, image_dim)\n",
    "\n",
    "with open(templates_valid_fname, 'rb') as f:\n",
    "    Y_valid = np.fromfile(f, dtype=np.float32, count=num_valid_images * template_dim)\n",
    "    Y_valid = Y_valid.reshape(num_valid_images, template_dim)\n",
    "    \n",
    "# read the validation files\n",
    "with open(images_valid_fname, 'rb') as f:\n",
    "    X_valid_tmp = np.fromfile(f, dtype=np.uint8, count=num_valid_images * image_dim).astype(np.float32)\n",
    "    X_valid_tmp = X_valid.reshape(num_valid_images, image_dim)\n",
    "\n",
    "\n",
    "# read the test file\n",
    "with open(images_test_fname, 'rb') as f:\n",
    "    X_test = np.fromfile(f, dtype=np.uint8, count=num_test_images * image_dim).astype(np.float32)\n",
    "    X_test = X_test.reshape(num_test_images, image_dim)\n",
    "    \n",
    "# read the validation files\n",
    "with open(templates_valid_fname, 'rb') as f:\n",
    "    valid_template_data = np.fromfile(f, dtype=np.float32, count=num_valid_images * template_dim)\n",
    "    valid_template_data = valid_template_data.reshape(num_valid_images, template_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X_train : (100000, 2304)\n",
      "Shape Y_train : (100000, 128)\n",
      "\n",
      "Shape X_valid : (10000, 2304)\n",
      "Shape Y_valid: (10000, 128)\n",
      "\n",
      "Shape X_test : (10000, 2304)\n"
     ]
    }
   ],
   "source": [
    "# Investigation of the shape of the data\n",
    "print(\"Shape X_train : {}\".format(X_train.shape))\n",
    "print(\"Shape Y_train : {}\\n\".format(Y_train.shape))\n",
    "print(\"Shape X_valid : {}\".format(X_valid.shape))\n",
    "print(\"Shape Y_valid: {}\\n\".format(Y_valid.shape))\n",
    "print(\"Shape X_test : {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsMAAAC0CAYAAABxNCESAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhI3IPwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119cef198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exemple of data :\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(12, 4))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(X_train[i].reshape(48, 48), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Centering-Reduction and Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Real-time data preprocessing\n",
    "img_prep = ImagePreprocessing()\n",
    "img_prep.add_featurewise_zero_center()\n",
    "img_prep.add_featurewise_stdnorm()\n",
    "\n",
    "# Real-time data augmentation\n",
    "img_aug = ImageAugmentation()\n",
    "img_aug.add_random_flip_leftright()\n",
    "img_aug.add_random_rotation(max_angle=25.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Performance measure (MSE)\n",
    "def compute_pred_score(y_true, y_pred):\n",
    "    err_y = np.mean((y_true - y_pred) ** 2)\n",
    "    return err_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Prediction\n",
    "\n",
    "In the cells below, we apply a simple linear regression that is given as follows:\n",
    "\n",
    "$\\textbf{Y}_{train} \\approx \\textbf{X}_{train} \\textbf{W}$ \n",
    "\n",
    "where our aim is to learn the matrix $\\textbf{W}$ from the training set. Then, by using $\\textbf{W}$, we can obtain the prediction on the test set, as follows:\n",
    "\n",
    "\n",
    "$\\hat{\\textbf{Y}}_{test} = \\textbf{X}_{test} \\textbf{W}$ \n",
    "\n",
    "The procedure is given below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simple linear regression \n",
    "\n",
    "# Learn the matrix W from the training data\n",
    "W = np.linalg.solve(np.dot(X_train.T, X_train),\n",
    "                    np.dot(X_train.T, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on the training data : 0.00722445\n",
      "Error on the validation data : 0.00781217\n"
     ]
    }
   ],
   "source": [
    "# Training error\n",
    "y_pred_train = np.dot(X_train, W)\n",
    "err_train = compute_pred_score(Y_train, y_pred_train)\n",
    "\n",
    "print('Error on the training data : %s' % err_train)\n",
    "\n",
    "# Monitor the validation error\n",
    "y_pred_valid = np.dot(X_valid, W)\n",
    "err_valid = compute_pred_score(Y_valid, y_pred_valid)\n",
    "\n",
    "print('Error on the validation data : %s' % err_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN networks with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution and pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolution(x, W, b):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def max_pooling(x, k=2):  \n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):  # Weight W initialization\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):  # Bias b initialization\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Convolutional network building\n",
    "    \n",
    "    # 1st Convolutional Layer\n",
    "    with tf.variable_scope('conv1'):\n",
    "        x = tf.reshape(x, shape=[-1, 48, 48, 1])\n",
    "        prep_x = input_data(placeholder=x, data_preprocessing=img_prep, data_augmentation=img_aug)\n",
    "        h_conv1 = convolution(prep_x, weights['wc1'], biases['bc1'])\n",
    "    with tf.variable_scope('pool1'): #1st pooling\n",
    "        h_pool1 = max_pooling(h_conv1)\n",
    "    \n",
    "    \n",
    "    # 2nd Convolutional Layer\n",
    "    with tf.variable_scope('conv2'):\n",
    "        h_conv2 = convolution(h_pool1, weights['wc2'],  biases['bc2'])\n",
    "    with tf.variable_scope('pool2'):\n",
    "        h_pool2 = max_pooling(h_conv2) #2nd pooling\n",
    "    \n",
    "    \n",
    "    # 3rd Convolutional Layer\n",
    "    with tf.variable_scope('conv3'):\n",
    "        h_conv3 = convolution(h_pool2, weights['wc3'], biases['bc3'])\n",
    "    with tf.variable_scope('pool3'):\n",
    "        h_pool3 = max_pooling(h_conv3) #3rd pooling\n",
    "   \n",
    "    # Fully (Densely) Connected Layer \n",
    "    with tf.variable_scope('fc'):\n",
    "        h_pool3_flat = tf.reshape(h_pool3, [-1, weights['wf1'].get_shape().as_list()[0]])\n",
    "        h_fc1 = tf.nn.softmax((tf.matmul(h_pool3_flat, weights['wf1']) + biases['bf1']))\n",
    "        # dropout\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, dropout)\n",
    "    \n",
    "    # with tf.variable_scope('Readout_layer') as scope:\n",
    "    # Readout layer \n",
    "    with tf.variable_scope('output'):\n",
    "        out = tf.matmul(h_fc1_drop, weights['wout']) + biases['bout']\n",
    "    tf.summary.histogram('output', out)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for spliting images into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class images_container():\n",
    "\n",
    "    def __init__(self, _X, _y, _batch_size):\n",
    "        self.current_position = 0\n",
    "        self.batch_size = _batch_size\n",
    "        \n",
    "        self.X = _X\n",
    "        self.y = _y\n",
    "        \n",
    "    def next_batch(self):\n",
    "        if(self.current_position + self.batch_size > len(self.X)):\n",
    "            self.current_position = 0\n",
    "        \n",
    "        X_batch = self.X[self.current_position: self.current_position + self.batch_size]\n",
    "        y_batch = self.y[self.current_position: self.current_position + self.batch_size]\n",
    "            \n",
    "        self.current_position += self.batch_size\n",
    "        return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to get the total number of parameters of the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_total_param():\n",
    "    \"\"\"\n",
    "    Be sure you are not over the 50 K !!!\n",
    "    \"\"\"\n",
    "    total_parameters = 0\n",
    "    #iterating over all variables\n",
    "    for variable in tf.trainable_variables():  \n",
    "        local_parameters=1\n",
    "        shape = variable.get_shape()  #getting shape of a variable\n",
    "        for i in shape:\n",
    "            local_parameters*=i.value  #mutiplying dimension values\n",
    "        total_parameters+=local_parameters\n",
    "    \n",
    "    return total_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st Attempt : Architecture with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters : 66110\n"
     ]
    }
   ],
   "source": [
    "# Graph inputs\n",
    "with tf.name_scope('data'):\n",
    "    X = tf.placeholder(tf.float32, [None, image_dim], name=\"X_placeholder\")\n",
    "    Y = tf.placeholder(tf.float32, [None, template_dim], name=\"Y_placeholder\")\n",
    "dropout = tf.placeholder(tf.float32, name=\"dropout\") \n",
    "\n",
    "tf.summary.scalar('dropout_keep_probability', dropout)\n",
    "\n",
    "# Weights and biases\n",
    "weights = {\n",
    "    # Image\n",
    "    # 1st conv. layer : 4x4 filter taking 1 input (image) and generating 10 outputs\n",
    "    'wc1': weight_variable([4, 4, 1, 10]),\n",
    "    # Pooling\n",
    "    # 2nd conv. layer : 4x4 filter, 10 inputs, 10 outputs\n",
    "    'wc2': weight_variable([4, 4, 10, 10]),\n",
    "    # 3rd conv. layer : 4x4 filter, 10 inputs, 10 outputs\n",
    "    'wc3': weight_variable([4, 4, 10, 10]),\n",
    "    # fully connected, 6*6*6 inputs, 128 outputs\n",
    "    'wf1': weight_variable([6 * 6 * 10, 128]),\n",
    "    # Readout layer\n",
    "    'wout': weight_variable([128, 128])\n",
    "}\n",
    "\n",
    "# Building the biases\n",
    "biases = {\n",
    "    'bc1': bias_variable([10]),\n",
    "    'bc2': bias_variable([10]),\n",
    "    'bc3': bias_variable([10]),\n",
    "    'bf1': bias_variable([128]),\n",
    "    'bout': bias_variable([128])\n",
    "}\n",
    "\n",
    "# Building the model\n",
    "pred = conv_net(X, weights, biases, dropout)\n",
    "\n",
    "# Get total number of parameters\n",
    "print(\"Total number of parameters : {}\".format(get_total_param()))\n",
    "\n",
    "# Loss function\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_sum(tf.square(Y - pred))\n",
    "tf.summary.scalar('loss', loss)\n",
    "\n",
    "# Merge all summaries \n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# Optimizer\n",
    "start_learning_rate = 0.01\n",
    "decay_steps = 1000\n",
    "decay_rate = 0.95\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "# Decaying learning rate\n",
    "learning_rate = tf.train.exponential_decay(start_learning_rate, global_step, decay_steps, decay_rate)\n",
    "\n",
    "# Choosing the AdamOptimiser\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runing the first attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (10/781) ==> Training loss: 142.11639404296875, Validation loss: 0.009764272719621658\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (20/781) ==> Training loss: 157.31243896484375, Validation loss: 0.009323719888925552\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (30/781) ==> Training loss: 126.50221252441406, Validation loss: 0.008611529134213924\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (40/781) ==> Training loss: 144.74929809570312, Validation loss: 0.009035570546984673\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (50/781) ==> Training loss: 131.12704467773438, Validation loss: 0.008515680208802223\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (60/781) ==> Training loss: 131.0172576904297, Validation loss: 0.008418726734817028\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (70/781) ==> Training loss: 135.82188415527344, Validation loss: 0.008652017451822758\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (80/781) ==> Training loss: 130.20584106445312, Validation loss: 0.008433469571173191\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (90/781) ==> Training loss: 127.65216064453125, Validation loss: 0.008427336812019348\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (100/781) ==> Training loss: 124.03289794921875, Validation loss: 0.008509221486747265\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (110/781) ==> Training loss: 132.0809326171875, Validation loss: 0.008461213670670986\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (120/781) ==> Training loss: 130.67098999023438, Validation loss: 0.008321174420416355\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (130/781) ==> Training loss: 129.89410400390625, Validation loss: 0.008327252231538296\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (140/781) ==> Training loss: 126.43820190429688, Validation loss: 0.008274347521364689\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (1/1) - Batch (150/781) ==> Training loss: 129.18536376953125, Validation loss: 0.008429383859038353\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (160/781) ==> Training loss: 129.8027801513672, Validation loss: 0.008462272584438324\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (170/781) ==> Training loss: 140.72775268554688, Validation loss: 0.008521630428731441\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (180/781) ==> Training loss: 127.57823944091797, Validation loss: 0.008353822864592075\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (190/781) ==> Training loss: 134.43585205078125, Validation loss: 0.00882836151868105\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (200/781) ==> Training loss: 135.0494384765625, Validation loss: 0.008732588030397892\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (210/781) ==> Training loss: 137.44757080078125, Validation loss: 0.008465739898383617\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (220/781) ==> Training loss: 132.515869140625, Validation loss: 0.008383381180465221\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (230/781) ==> Training loss: 130.6971435546875, Validation loss: 0.008290129713714123\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (240/781) ==> Training loss: 134.20584106445312, Validation loss: 0.008934466168284416\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (250/781) ==> Training loss: 130.93267822265625, Validation loss: 0.008496694266796112\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (260/781) ==> Training loss: 112.34268188476562, Validation loss: 0.0086147990077734\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (270/781) ==> Training loss: 131.353515625, Validation loss: 0.009591289795935154\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (280/781) ==> Training loss: 131.457275390625, Validation loss: 0.008692287839949131\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (290/781) ==> Training loss: 133.19009399414062, Validation loss: 0.008855832740664482\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (300/781) ==> Training loss: 129.6513671875, Validation loss: 0.008351209573447704\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (310/781) ==> Training loss: 138.78021240234375, Validation loss: 0.008807129226624966\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (320/781) ==> Training loss: 126.67327880859375, Validation loss: 0.008389465510845184\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (330/781) ==> Training loss: 146.1182861328125, Validation loss: 0.008669637143611908\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (340/781) ==> Training loss: 139.68780517578125, Validation loss: 0.008633311837911606\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (350/781) ==> Training loss: 137.38714599609375, Validation loss: 0.008689570240676403\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (360/781) ==> Training loss: 135.5696258544922, Validation loss: 0.008384852670133114\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (370/781) ==> Training loss: 120.40206909179688, Validation loss: 0.00847159419208765\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (380/781) ==> Training loss: 134.39883422851562, Validation loss: 0.008784639649093151\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (390/781) ==> Training loss: 138.1127166748047, Validation loss: 0.008683493360877037\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (400/781) ==> Training loss: 143.9817657470703, Validation loss: 0.008947120979428291\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (410/781) ==> Training loss: 144.30894470214844, Validation loss: 0.008891905657947063\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (420/781) ==> Training loss: 99.1552734375, Validation loss: 0.009430604055523872\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (430/781) ==> Training loss: 155.27784729003906, Validation loss: 0.009297187440097332\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (440/781) ==> Training loss: 131.68478393554688, Validation loss: 0.008640107698738575\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (450/781) ==> Training loss: 147.24462890625, Validation loss: 0.00857316330075264\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (1/1) - Batch (460/781) ==> Training loss: 136.86788940429688, Validation loss: 0.00871044397354126\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (470/781) ==> Training loss: 134.80511474609375, Validation loss: 0.008465497754514217\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (480/781) ==> Training loss: 129.043212890625, Validation loss: 0.008401894010603428\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (490/781) ==> Training loss: 144.66946411132812, Validation loss: 0.008409073576331139\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (500/781) ==> Training loss: 136.25369262695312, Validation loss: 0.008314767852425575\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (510/781) ==> Training loss: 133.01573181152344, Validation loss: 0.008629627525806427\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (520/781) ==> Training loss: 132.17947387695312, Validation loss: 0.008690518327057362\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (530/781) ==> Training loss: 127.95002746582031, Validation loss: 0.00826176255941391\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (540/781) ==> Training loss: 137.84024047851562, Validation loss: 0.00854706484824419\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (550/781) ==> Training loss: 131.30068969726562, Validation loss: 0.008160432800650597\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (560/781) ==> Training loss: 134.13742065429688, Validation loss: 0.008553712628781796\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (570/781) ==> Training loss: 138.9590301513672, Validation loss: 0.008485150523483753\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (580/781) ==> Training loss: 131.20626831054688, Validation loss: 0.00828532874584198\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (590/781) ==> Training loss: 125.95951843261719, Validation loss: 0.00823221169412136\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (600/781) ==> Training loss: 120.76087951660156, Validation loss: 0.008660239167511463\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (610/781) ==> Training loss: 125.7355728149414, Validation loss: 0.008532008156180382\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (620/781) ==> Training loss: 107.97467041015625, Validation loss: 0.00872641708701849\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (630/781) ==> Training loss: 124.18270111083984, Validation loss: 0.008502629585564137\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (640/781) ==> Training loss: 138.05828857421875, Validation loss: 0.00881904922425747\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (650/781) ==> Training loss: 131.63092041015625, Validation loss: 0.008151345886290073\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (660/781) ==> Training loss: 139.8719024658203, Validation loss: 0.009503563866019249\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (670/781) ==> Training loss: 129.56085205078125, Validation loss: 0.008366101421415806\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (680/781) ==> Training loss: 140.6306610107422, Validation loss: 0.008424638770520687\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (690/781) ==> Training loss: 130.99453735351562, Validation loss: 0.00831469614058733\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (700/781) ==> Training loss: 106.9515609741211, Validation loss: 0.008625112473964691\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (710/781) ==> Training loss: 151.57797241210938, Validation loss: 0.009679646231234074\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (720/781) ==> Training loss: 139.31658935546875, Validation loss: 0.008457113988697529\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (730/781) ==> Training loss: 141.06686401367188, Validation loss: 0.008393775671720505\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (740/781) ==> Training loss: 129.3287353515625, Validation loss: 0.008367421105504036\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (750/781) ==> Training loss: 116.525146484375, Validation loss: 0.008515221998095512\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (760/781) ==> Training loss: 146.14627075195312, Validation loss: 0.008567137643694878\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (1/1) - Batch (770/781) ==> Training loss: 133.4870147705078, Validation loss: 0.008277414366602898\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImagePreprocessing' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'ImageAugmentation' object has no attribute 'name'\n",
      "Epoch (1/1) - Batch (780/781) ==> Training loss: 121.59902954101562, Validation loss: 0.008403412997722626\n",
      "Optimization finished\n",
      "CPU times: user 30min 35s, sys: 11min 5s, total: 41min 41s\n",
      "Wall time: 16min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Simulation parameters\n",
    "BATCH_SIZE = 128\n",
    "DISPLAY_STEP = 10\n",
    "DROPOUT = 0.75\n",
    "n_batches = int(num_train_images / BATCH_SIZE)  # to process all the training dataset (781 batches in total)\n",
    "N_EPOCHS = 1  # train the model N_EPOCHS times\n",
    "\n",
    "# Train the model, write training summaries by adding them to the summary_writer \n",
    "with tf.Session() as sess: \n",
    "    sess.run(tf.global_variables_initializer())  # Run session and initializing variables\n",
    "    saver = tf.train.Saver() # To save the model \n",
    "    writer = tf.summary.FileWriter(\"./log_files\", sess.graph)  # Generate tons of logs (including summaries)\n",
    "\n",
    "    train_data = images_container(X_train, Y_train, BATCH_SIZE)\n",
    "    step = 1\n",
    "    \n",
    "    # Keep training until reach max iterations\n",
    "    while step < (N_EPOCHS * n_batches):  # 3905 iterations\n",
    "        batch_X, batch_Y = train_data.next_batch()\n",
    "        # Run optimization op (backprop) and calculate batch loss \n",
    "        # _, loss_train, summary = sess.run([optimizer, loss, merged], feed_dict={X: batch_X, Y: batch_Y, dropout: 1})\n",
    "        sess.run(optimizer, feed_dict={X: batch_X, Y: batch_Y, dropout: DROPOUT})\n",
    "\n",
    "        # lr = sess.run(learning_rate)\n",
    "        # print(lr)\n",
    "        if step % DISPLAY_STEP == 0:\n",
    "            loss_train, summary = sess.run([loss, merged], feed_dict={X: batch_X, Y: batch_Y, dropout: 1.0}) \n",
    "            writer.add_summary(summary, step)\n",
    "            \n",
    "            saver.save(sess, \"./log_files/model.ckpt\", step)\n",
    "            \n",
    "            # Make predictions and compute score on validation set\n",
    "            Y_valid_pred = sess.run(pred, {X: X_valid, dropout: 1.0})\n",
    "            # Look at the score on validation set\n",
    "            loss_valid = compute_pred_score(Y_valid_pred, Y_valid)\n",
    "            print(\"Epoch ({}/{}) - Batch ({}/{}) ==> Training loss: {}, Validation loss: {}\".format(int(step / n_batches) + 1, \n",
    "                                                    N_EPOCHS, np.mod(step, n_batches), n_batches, loss_train, loss_valid))\n",
    "            \n",
    "        step += 1\n",
    "        \n",
    "    print(\"Optimization finished\")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runing the predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\t\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"./log_files/model.ckpt-1960\")\n",
    "\n",
    "train_data = images_container(X_train, Y_train, BATCH_SIZE)\n",
    "step = 1\n",
    "# Keep training until reach max iterations\n",
    "while step < TRAINING_ITERS:\n",
    "    batch_X, batch_Y = train_data.next_batch()\n",
    "    # Run optimization op (backprop) and calculate batch loss \n",
    "    _, loss_train, summary = sess.run([optimizer, loss, merged], feed_dict={X: batch_X, Y: batch_Y, dropout: 0.75})\n",
    "        \n",
    "    if step % DISPLAY_STEP == 0:\n",
    "        writer.add_summary(summary, step)\n",
    "        saver.save(sess, \"./log_files/model.ckpt-1960\", step)\n",
    "            \n",
    "        # Make predictions and compute score on validation set\n",
    "        Y_valid_pred = sess.run(pred, {X: X_valid, dropout: 1})\n",
    "        # Look at the score on validation set\n",
    "        loss_valid = compute_pred_score(Y_valid_pred, Y_valid)\n",
    "        print(\"Iteration {}, Training loss: {}, Validation loss: {}\".format(step, loss_train, loss_valid))\n",
    "            \n",
    "    step += 1\n",
    "        \n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runing the model and saving the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\t\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"./log_files/model.ckpt\")\n",
    "\n",
    "Y_test = sess.run(pred, {X: X_test, dropout: 1.0})\n",
    "\n",
    "sess.close()\n",
    "\n",
    "# Write prediction in test file\n",
    "f = open('template_pred.bin', 'wb')\n",
    "for i in range(num_test_images):\n",
    "    f.write(Y_test[i, :])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, Activation, LocallyConnected2D, AveragePooling2D, Dropout, MaxPooling2D\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st Convolutional Layer\n",
    "model.add(Conv2D(10,kernel_size=(4, 4),input_shape=(48, 48, 1)\n",
    "                 ,padding='same', kernel_initializer='glorot_normal', name='Conv1'))\n",
    "model.add(PReLU(shared_axes=[1,2]))\n",
    "model.add(AveragePooling2D(pool_size=(2,2), name='pool1'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "model.add(Conv2D(10,kernel_size=(4, 4), padding='same'\n",
    "                 , kernel_initializer='glorot_normal', name='Conv2'))\n",
    "model.add(PReLU(shared_axes=[1,2]))\n",
    "model.add(AveragePooling2D(pool_size=(2,2), name='pool2'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# 3rd Convolutional Layer\n",
    "model.add(Conv2D(10,kernel_size=(4, 4), padding='same'\n",
    "                 , kernel_initializer='glorot_normal', name='Conv3'))\n",
    "model.add(PReLU(shared_axes=[1,2]))\n",
    "model.add(AveragePooling2D(pool_size=(2,2), name='pool3'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(LocallyConnected2D(filters=128, kernel_size=(6,6), strides=(1,1)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1 (Conv2D)               (None, 48, 48, 10)        170       \n",
      "_________________________________________________________________\n",
      "p_re_lu_4 (PReLU)            (None, 48, 48, 10)        10        \n",
      "_________________________________________________________________\n",
      "pool1 (AveragePooling2D)     (None, 24, 24, 10)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 24, 24, 10)        0         \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 24, 24, 10)        1610      \n",
      "_________________________________________________________________\n",
      "p_re_lu_5 (PReLU)            (None, 24, 24, 10)        10        \n",
      "_________________________________________________________________\n",
      "pool2 (AveragePooling2D)     (None, 12, 12, 10)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 12, 12, 10)        0         \n",
      "_________________________________________________________________\n",
      "Conv3 (Conv2D)               (None, 12, 12, 10)        1610      \n",
      "_________________________________________________________________\n",
      "p_re_lu_6 (PReLU)            (None, 12, 12, 10)        10        \n",
      "_________________________________________________________________\n",
      "pool3 (AveragePooling2D)     (None, 6, 6, 10)          0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 6, 6, 10)          0         \n",
      "_________________________________________________________________\n",
      "locally_connected2d_2 (Local (None, 1, 1, 128)         46208     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 128)               0         \n",
      "=================================================================\n",
      "Total params: 49,628\n",
      "Trainable params: 49,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to compute the score with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_keras_pred_score(y_true, y_pred):\n",
    "    err_y = keras.backend.mean((y_true - y_pred) ** 2)*100.\n",
    "    return err_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runing the keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape([-1,48,48,1])\n",
    "X_valid = X_valid.reshape([-1,48,48,1])\n",
    "X_test = X_test.reshape([-1,48,48,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Conv1_24/kernel:0 is illegal; using Conv1_24/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name Conv1_24/kernel:0 is illegal; using Conv1_24/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name Conv1_24/bias:0 is illegal; using Conv1_24/bias_0 instead.\n",
      "INFO:tensorflow:Summary name Conv1_24/bias:0 is illegal; using Conv1_24/bias_0 instead.\n",
      "INFO:tensorflow:Summary name p_re_lu_1_23/alpha:0 is illegal; using p_re_lu_1_23/alpha_0 instead.\n",
      "INFO:tensorflow:Summary name p_re_lu_1_23/alpha:0 is illegal; using p_re_lu_1_23/alpha_0 instead.\n",
      "INFO:tensorflow:Summary name Conv2_24/kernel:0 is illegal; using Conv2_24/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name Conv2_24/kernel:0 is illegal; using Conv2_24/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name Conv2_24/bias:0 is illegal; using Conv2_24/bias_0 instead.\n",
      "INFO:tensorflow:Summary name Conv2_24/bias:0 is illegal; using Conv2_24/bias_0 instead.\n",
      "INFO:tensorflow:Summary name p_re_lu_2_23/alpha:0 is illegal; using p_re_lu_2_23/alpha_0 instead.\n",
      "INFO:tensorflow:Summary name p_re_lu_2_23/alpha:0 is illegal; using p_re_lu_2_23/alpha_0 instead.\n",
      "INFO:tensorflow:Summary name Conv3_24/kernel:0 is illegal; using Conv3_24/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name Conv3_24/kernel:0 is illegal; using Conv3_24/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name Conv3_24/bias:0 is illegal; using Conv3_24/bias_0 instead.\n",
      "INFO:tensorflow:Summary name Conv3_24/bias:0 is illegal; using Conv3_24/bias_0 instead.\n",
      "INFO:tensorflow:Summary name p_re_lu_3_23/alpha:0 is illegal; using p_re_lu_3_23/alpha_0 instead.\n",
      "INFO:tensorflow:Summary name p_re_lu_3_23/alpha:0 is illegal; using p_re_lu_3_23/alpha_0 instead.\n",
      "INFO:tensorflow:Summary name locally_connected2d_1_23/kernel:0 is illegal; using locally_connected2d_1_23/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name locally_connected2d_1_23/kernel:0 is illegal; using locally_connected2d_1_23/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name locally_connected2d_1_23/bias:0 is illegal; using locally_connected2d_1_23/bias_0 instead.\n",
      "INFO:tensorflow:Summary name locally_connected2d_1_23/bias:0 is illegal; using locally_connected2d_1_23/bias_0 instead.\n",
      "Epoch 1/1\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 0.0071Epoch 00000: saving model to ./logs/weights.hdf5\n",
      "100000/100000 [==============================] - 348s - loss: 0.0071   \n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model_keras.h5')\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Learning rate with decay\n",
    "adam = Adam(lr=0.001, decay=1e-6)\n",
    "model.compile(loss='mean_squared_error',optimizer=adam)\n",
    "\n",
    "batch_size = 128\n",
    "nb_epoch = 50\n",
    "\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(filepath=\"./logs/weights.hdf5\",\n",
    "                                               verbose=1, save_best_only=False, mode='auto', period=1)\n",
    "\n",
    "# Saving on tensorboard\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph',\n",
    "                                         histogram_freq=1,write_graph=True, write_images=True)\n",
    "\n",
    "# Fiting the model and saving the loss\n",
    "history = model.fit(X_train, Y_train,batch_size=batch_size,\n",
    "                    epochs=nb_epoch,verbose=1,callbacks=[tbCallBack, checkpointer])\n",
    "\n",
    "\n",
    "# creates a HDF5 file 'my_model.h5'\n",
    "model.save('model_keras.h5')\n",
    "\n",
    "# returns a compiled model\n",
    "model = load_model('model_keras.h5')\n",
    "\n",
    "# Prediction\n",
    "Y_test = model.predict(X_test)\n",
    "f = open('model_keras.bin', 'wb')\n",
    "for i in range(num_test_images):\n",
    "    f.write(Y_test[i, :])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaUAAAEWCAYAAADGjIh1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSpvRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c6c0710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
